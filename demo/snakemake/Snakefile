"""
Snakemake x Io (DataDAG) example pipeline
"""
import os
import glob
import pandas as pd
import seaborn as sns

from iodat.packager import Packager

rule all:
    input:
        "results/exclude_missing/penguins.csv",
        "results/exclude_missing/datapackage.json"

rule fetch_data:
    output:
        "results/raw/penguins.csv",
        "results/raw/datapackage.json"
    run:
        # in the first step of this toy "pipeline", all we are doing is downloading the
        # data and saving it as-is
        # here, seaborn's "load_dataset" function is used to retrieve a dataframe
        # representation of the Palmers Penguins dataset
        dat = sns.load_dataset('penguins')

        # below are the steps relating used to describe and generate an initial Io
        # datapackage

        # first, we create an Io Packager instance
        pkgr = Packager()

        # directory where we want to output our data package to
        out_dir = os.path.dirname(output[0])

        # each datapackage can include one or more "resources"; this is where our
        # dataframe goes
        # the name we provide here will be used as its key in the datapackage
        # "resources" section
        resources = {
            "penguins": dat,
        }

        # annotations can optionally be provided as paths to external markdown, etc.
        # plain-text files, or, as string annotations
        annot = ["annot/fetch_data/overview.md"]

        # views can be provided as paths to external vega-lite json files, or, as dict
        # representations of such a view
        views = ["views/fetch_data/scatterplot.json"]

        # metadata can be specified in two different ways:
        # 1. the "metadata" function param, used to specific _node-level_ metadata
        # 2. kwargs, used to specify _workflow-_ or _dag-level_ metadata

        # node-level metadata
        metadata = {
            "title": "Unprocessed data",
            "description": "Data downloaded via Seaborn, with no additional processing applied.",
            "source": {
                "name": "Gorman et al. (2014)",
                "date": "2022-01-07",
                "urls": ["https://allisonhorst.github.io/palmerpenguins/"],
                "description": "Data for penguins of 3 different species (Adelie, Chinstrap, and Gentoo) collected in the Palmer Archipelago, Antarctica.",
                "citations": ["10.1371/journal.pone.0090081"]
            }
        }

        # dag-level metadata
        contributors = {
            "title": "Gunter",
            "role": "maintainer"
        }

        # generate and save io data package
        pkgr.build_package(resources, annot, views, metadata, name="Raw Data",
                pkg_dir=out_dir, contributors=contributors)

rule exclude_missing:
    input:
        "results/raw/penguins.csv",
        "results/raw/datapackage.json"
    output:
        "results/exclude_missing/penguins.csv",
        "results/exclude_missing/datapackage.json"
    run:
        # subsequent stages of data processing look similar to the first, except...
        #.......
        # the other change is that we swap the call to "build_package" for
        # "update_package", and we provide a path to an _existing_ datapackage.json as
        # as the first argument the the function call
        dat = pd.read_csv(input[0])

        # in our toy example, the only change we are making to the data is to remove any
        # rows which contain missing values
        dat.dropna().to_csv(output[0])

        # create io packager instance and load recipe
        pkgr = Packager()

        # data
        resources = {
            "penguins": dat,
        }

        # generate data package and write to disk
        out_dir = os.path.dirname(output[0])

        # in this case, we don't have anything additional to describe, so not
        # views/annotations are included in the call to "update_package"
        existing_pkg = input[1]

        metadata = {
            "title": "Filtered data",
            "description": "Rows with missing values have been excluded."
        }

        pkg = pkgr.update_package(existing_pkg, resources, metadata=metadata, pkg_dir=out_dir)
